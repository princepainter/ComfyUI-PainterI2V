import torch
import comfy.model_management
import comfy.utils
import node_helpers
from comfy_api.latest import io, ComfyExtension
from typing_extensions import override

class PainterI2V(io.ComfyNode):
    """
    Wan2.2 å›¾ç”Ÿè§†é¢‘å¢å¼ºèŠ‚ç‚¹ - è§£å†³4æ­¥LoRAæ…¢åŠ¨ä½œé—®é¢˜
    ä¸“ä¸ºå•å¸§è¾“å…¥ä¼˜åŒ–ï¼Œæå‡è¿åŠ¨å¹…åº¦ï¼Œä¿æŒç”»é¢äº®åº¦ç¨³å®š
    """
    
    @classmethod
    def define_schema(cls):
        return io.Schema(
            node_id="PainterI2V",
            category="conditioning/video_models",
            inputs=[
                io.Conditioning.Input("positive"),
                io.Conditioning.Input("negative"),
                io.Vae.Input("vae"),
                io.Int.Input("width", default=832, min=16, max=4096, step=16),
                io.Int.Input("height", default=480, min=16, max=4096, step=16),
                io.Int.Input("length", default=81, min=1, max=4096, step=4),
                io.Int.Input("batch_size", default=1, min=1, max=4096),
                io.Float.Input("motion_amplitude", default=1.15, min=1.0, max=2.0, step=0.05),
                io.ClipVisionOutput.Input("clip_vision_output", optional=True),
                io.Image.Input("start_image", optional=True),
            ],
            outputs=[
                io.Conditioning.Output(display_name="positive"),
                io.Conditioning.Output(display_name="negative"),
                io.Latent.Output(display_name="latent"),
            ]
        )

    @classmethod
    def execute(cls, positive, negative, vae, width, height, length, batch_size,
                motion_amplitude=1.15, start_image=None, clip_vision_output=None) -> io.NodeOutput:
        # 1. ä¸¥æ ¼çš„é›¶latentåˆå§‹åŒ–ï¼ˆ4æ­¥LoRAçš„ç”Ÿå‘½çº¿ï¼‰
        latent = torch.zeros([batch_size, 16, ((length - 1) // 4) + 1, height // 8, width // 8], 
                           device=comfy.model_management.intermediate_device())
        
        if start_image is not None:
            # å•å¸§è¾“å…¥å¤„ç†
            start_image = start_image[:1]
            start_image = comfy.utils.common_upscale(
                start_image.movedim(-1, 1), width, height, "bilinear", "center"
            ).movedim(1, -1)
            
            # åˆ›å»ºåºåˆ—ï¼šé¦–å¸§çœŸå®ï¼Œåç»­0.5ç°
            image = torch.ones((length, height, width, start_image.shape[-1]), 
                             device=start_image.device, dtype=start_image.dtype) * 0.5
            image[0] = start_image[0]
            
            concat_latent_image = vae.encode(image[:, :, :, :3])
            
            # å•å¸§maskï¼šä»…çº¦æŸé¦–å¸§
            mask = torch.ones((1, 1, latent.shape[2], concat_latent_image.shape[-2], 
                             concat_latent_image.shape[-1]), 
                            device=start_image.device, dtype=start_image.dtype)
            mask[:, :, 0] = 0.0
            
            # 2. è¿åŠ¨å¹…åº¦å¢å¼ºï¼ˆäº®åº¦ä¿æŠ¤æ ¸å¿ƒç®—æ³•ï¼‰
            if motion_amplitude > 1.0:
                base_latent = concat_latent_image[:, :, 0:1]      # é¦–å¸§
                gray_latent = concat_latent_image[:, :, 1:]       # ç°å¸§
                
                diff = gray_latent - base_latent
                diff_mean = diff.mean(dim=(1, 3, 4), keepdim=True)
                diff_centered = diff - diff_mean
                scaled_latent = base_latent + diff_centered * motion_amplitude + diff_mean
                
                # Clamp & ç»„åˆ
                scaled_latent = torch.clamp(scaled_latent, -6, 6)
                concat_latent_image = torch.cat([base_latent, scaled_latent], dim=2)
            
            # 3. æ³¨å…¥åˆ°conditioning
            positive = node_helpers.conditioning_set_values(
                positive, {"concat_latent_image": concat_latent_image, "concat_mask": mask}
            )
            negative = node_helpers.conditioning_set_values(
                negative, {"concat_latent_image": concat_latent_image, "concat_mask": mask}
            )

            # 4. å‚è€ƒå¸§å¢å¼º
            ref_latent = vae.encode(start_image[:, :, :, :3])
            positive = node_helpers.conditioning_set_values(positive, {"reference_latents": [ref_latent]}, append=True)
            negative = node_helpers.conditioning_set_values(negative, {"reference_latents": [torch.zeros_like(ref_latent)]}, append=True)

        if clip_vision_output is not None:
            positive = node_helpers.conditioning_set_values(positive, {"clip_vision_output": clip_vision_output})
            negative = node_helpers.conditioning_set_values(negative, {"clip_vision_output": clip_vision_output})

        out_latent = {}
        out_latent["samples"] = latent
        return io.NodeOutput(positive, negative, out_latent)


class PainterI2VExtension(ComfyExtension):
    @override
    async def get_node_list(self) -> list[type[io.ComfyNode]]:
        return [PainterI2V]

async def comfy_entrypoint() -> PainterI2VExtension:
    return PainterI2VExtension()


# èŠ‚ç‚¹æ³¨å†Œæ˜ å°„
NODE_CLASS_MAPPINGS = {
    "PainterI2V": PainterI2V,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PainterI2V": "ğŸ¨ PainterI2V (Wan2.2 æ…¢åŠ¨ä½œä¿®å¤)",
}
